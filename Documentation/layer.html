<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.5.4" />
<title>DenserFlow.layer API documentation</title>
<meta name="description" content="Defines neural network layers. A layer in a network
has various attributes - see the Layer parent class
for a description of each main function." />
<link href='https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css' rel='stylesheet'>
<link href='https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/8.0.0/sanitize.min.css' rel='stylesheet'>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">
<style>.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{font-weight:bold}#index h4 + ul{margin-bottom:.6em}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.name small{font-weight:normal}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase;cursor:pointer}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title"><code>DenserFlow.layer</code> module</h1>
</header>
<section id="section-intro">
<p>Defines neural network layers. A layer in a network
has various attributes - see the Layer parent class
for a description of each main function.</p>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">&#34;&#34;&#34;
Defines neural network layers. A layer in a network
has various attributes - see the Layer parent class
for a description of each main function.
&#34;&#34;&#34;
from typing import Optional
import numpy as np
from nptyping import Array

from .activation import Activation, linear, logistic, relu, leaky_relu


class Layer(object):
    &#34;&#34;&#34;
    The parent class for all layer classes.
    &#34;&#34;&#34;

    def __init__(self, activation: Activation = linear()):
        self.prev_activation = None
        self.activation = activation

    def _add_prev_layer(self, prev_activation: Activation, prev_out_dim: int) -&gt; None:
        &#34;&#34;&#34;
        Utility function used when creating a model. For our
        implementation of backpropagation, the activation function
        of the previous layer in the network must be known. This
        should not be called by the end user.
        &#34;&#34;&#34;
        self.prev_activation = prev_activation
        self.in_dim = prev_out_dim

    def get_activation(self) -&gt; Activation:
        &#34;&#34;&#34;
        Returns activation function for this layer.
        &#34;&#34;&#34;
        return self.activation

    def forward(self, input_batch: Array[float], mode: str = &#34;train&#34;) -&gt; Array[float]:
        &#34;&#34;&#34;
        Performs a forward pass given an input batch.
        :param input_batch: the input to the layer
        :param mode: whether we are training or testing the layer at this point.
        &#34;&#34;&#34;
        raise AttributeError(&#34;Unimplemented layer function&#34;)

    def backward(self, deltas: Array[float]) -&gt; Array[float]:
        &#34;&#34;&#34;
        Performs a backward pass given a delta.
        :param deltas: the delta passed through from the previous layer -
        the derivative of the error function with respect to the
        net of the current layer.
        &#34;&#34;&#34;
        raise AttributeError(&#34;Unimplemented layer function&#34;)

    def update(self, lr: float, wd: float, m: float) -&gt; None:
        &#34;&#34;&#34;
        Update the weights of this layer.
        :param lr: the learning rate of the model.
        :param wd: the weight decay rate of the model.
        :param m: the momentum rate of the model.
        &#34;&#34;&#34;
        raise AttributeError(&#34;Unimplemented layer functions&#34;)

    def update_adam(self, t: int, alpha: float, beta1: float, beta2: float) -&gt; None:
        &#34;&#34;&#34;
        Update the weights of this layer using the adam algorithm.
        :param t: the current timestep of the model (usually the current epoch number)
        :param alpha: the learning rate of the model.
        :param beta1: the decay rate for the first moment.
        :param beta2: the decay rate for the second moment.
        &#34;&#34;&#34;
        raise AttributeError(&#34;Unimplemented layer functions&#34;)


class DenseLayer(Layer):
    &#34;&#34;&#34;
     A fully connected layer. Weight matrix W should be of shape (n_in,n_out)
     and the bias vector b should be of shape (n_out,).

    Hidden unit activation is given by: f(dot(input,W) + b)

    &#34;&#34;&#34;

    def __init__(
        self,
        n_in: int,
        n_out: int,
        activation: Activation = linear(),
        W: Optional[Array[float]] = None,
        b: Optional[Array[float]] = None,
    ):
        &#34;&#34;&#34;
        :param n_in: dimensionality of input

        :param n_out: number of hidden units

        :param activation: Non linearity to be applied in the hidden layer

        :param W: Weight matrix to initialise weights with.
        If none, Xavier Initialisation is used.

        :param b: Bias matrix to initialise bias values with.
        If none, Xavier Initialisation is used.
        &#34;&#34;&#34;
        super().__init__(activation)
        # useful variable to store
        self.in_dim = n_in
        self.out_dim = n_out
        self.input = None

        # initialise weights and bias
        if W is None:
            # we use He initialisation if using relu
            if type(activation) is relu or type(activation) is leaky_relu:
                self.W = np.random.normal(
                    loc=0, scale=np.sqrt(2.0 / n_in), size=(n_in, n_out)
                )
            # else use xavier initialisation
            else:
                self.W = np.random.uniform(
                    low=-np.sqrt(6.0 / (n_in + n_out)),
                    high=np.sqrt(6.0 / (n_in + n_out)),
                    size=(n_in, n_out),
                )
            if type(activation) is logistic:
                self.W *= 4

        self.b = np.zeros((n_out))

        # initialise gradient matrices
        self.grad_W = np.zeros(self.W.shape)
        self.grad_b = np.zeros(self.b.shape)

        self.prev_grad_W = np.zeros(self.W.shape)
        self.prev_grad_b = np.zeros(self.b.shape)

        self.m_W = np.zeros(self.W.shape)
        self.v_W = np.zeros(self.W.shape)

        self.m_b = np.zeros(self.b.shape)
        self.v_b = np.zeros(self.b.shape)

    # utility function for forward pass below
    def _apply_weights(self, input_row):
        return np.dot(input_row, self.W)

    def forward(self, input_batch: Array[float], mode: str = &#34;train&#34;) -&gt; Array[float]:
        output = np.zeros((input_batch.shape[0], self.out_dim))
        # iterate over each sample in the minibatch
        lin_output = np.apply_along_axis(self._apply_weights, 1, input_batch) + self.b
        output = np.apply_along_axis(self.activation.f, 1, lin_output)
        # save values for backward pass
        self.output = output
        self.input = input_batch
        return self.output

    def backward(self, deltas: Array[float]) -&gt; Array[float]:
        # save previous gradients
        self.prev_grad_W = self.grad_W
        self.prev_grad_b = self.grad_b
        # accumulate gradients over the minibatch
        self.grad_W = np.zeros_like(self.W)
        self.grad_b = np.zeros_like(self.b)
        # prepare for next layer
        next_deltas = np.zeros((deltas.shape[0], self.in_dim))
        idx = 0
        self.grad_b += np.sum(deltas)
        for delta, sample in zip(deltas, self.input):
            self.grad_W += np.atleast_2d(sample).T.dot(np.atleast_2d(delta))
            if self.prev_activation is not None:
                next_deltas[idx] = delta.dot(self.W.T) * self.prev_activation.f_deriv(
                    sample
                )
            idx += 1
        return next_deltas

    def update_adam(self, t: int, alpha: float, beta1: float, beta2: float) -&gt; None:
        # adam code based off the fourth tutorial of comp5329. Adapted for our purposes.
        self.m_W = beta1 * self.m_W + (1 - beta1) * self.grad_W
        self.v_W = beta2 * self.v_W + (1 - beta2) * (self.grad_W ** 2)
        m_hat = self.m_W / (1 - beta1 ** t)
        v_hat = self.v_W / (1 - beta2 ** t)
        self.W = self.W - alpha * (m_hat / (np.sqrt(v_hat) - 1e-8))

        self.m_b = beta1 * self.m_b + (1 - beta1) * self.grad_b
        self.v_b = beta2 * self.v_b + (1 - beta2) * (self.grad_b ** 2)
        m_hat = self.m_b / (1 - beta1 ** t)
        v_hat = self.v_b / (1 - beta2 ** t)
        self.b = self.b - alpha * (m_hat / (np.sqrt(v_hat) - 1e-8))

    def update(self, lr: float, wd: float, m: float) -&gt; None:
        self.W -= (lr * (self.grad_W + m * self.prev_grad_W)) - (lr * wd * self.grad_W)
        self.b -= (lr * (self.grad_b + m * self.prev_grad_b)) - (lr * wd * self.grad_b)


class ActivationLayer(Layer):
    &#34;&#34;&#34;
    A layer that just applies an activation function. This can be used if you
    wish to apply an activation independent of a dense layer.
    &#34;&#34;&#34;

    def __init__(self, activation: Activation):
        &#34;&#34;&#34;
        :param activation: The activation function to use.
        &#34;&#34;&#34;
        self.activation = activation

    # we dont change the dimensions of the data in an activation layer,
    # so in dim == out dim.
    def _add_prev_layer(self, prev_activation: Activation, prev_out_dim: int) -&gt; None:
        self.prev_activation = prev_activation
        self.in_dim = prev_out_dim
        self.out_dim = prev_out_dim

    def forward(self, input_batch: Array[float], mode: str = &#34;train&#34;) -&gt; Array[float]:
        output = np.apply_along_axis(self.activation.f, 1, input_batch)
        # save values for backward pass
        self.output = output
        self.input = input_batch
        return output

    def backward(self, deltas: Array[float]) -&gt; Array[float]:
        # prepare for next layer
        next_deltas = np.zeros((deltas.shape[0], self.in_dim))
        idx = 0
        for delta, sample in zip(deltas, self.input):
            if self.prev_activation is not None:
                next_deltas[idx] = delta * self.prev_activation.f_deriv(sample)
            idx += 1
        return next_deltas

    def update(self, lr: float, wd: float, m: float) -&gt; None:
        pass

    def update_adam(self, t: int, alpha: float, beta1: float, beta2: float) -&gt; None:
        pass


class Dropout(Layer):
    &#34;&#34;&#34;
    A Dropout layer. This randomly drops nodes in the previous
    layer at a given probability rate.
    &#34;&#34;&#34;

    def __init__(self, dropout_rate: float):
        &#34;&#34;&#34;
        :param dropout_rate: The rate at which units should be dropped.
        Must be between 0 and 1.
        &#34;&#34;&#34;
        super().__init__()
        self.keep_rate = 1 - dropout_rate

    # we dont change the dimensions of the data in a dropout layer,
    # so in dim == out dim.
    def _add_prev_layer(self, prev_activation: Activation, prev_out_dim: int) -&gt; None:
        self.prev_activation = prev_activation
        self.in_dim = prev_out_dim
        self.out_dim = prev_out_dim

    def forward(self, input_batch: Array[float], mode: str = &#34;train&#34;) -&gt; Array[float]:
        self.dropout_matrix = (
            np.random.binomial(1, self.keep_rate, size=input_batch.shape)
            / self.keep_rate
        )
        return np.reshape(input_batch * self.dropout_matrix, input_batch.shape)

    def backward(self, deltas: Array[float]) -&gt; Array[float]:
        return deltas * self.dropout_matrix

    # nothing to learn, so nothing happens in update
    def update(self, lr: float, wd: float, m: float) -&gt; None:
        pass

    def update_adam(self, t: int, alpha: float, beta1: float, beta2: float) -&gt; None:
        pass


class BatchNormLayer(Layer):
    &#34;&#34;&#34;
    A Batch Normalisation layer. Performs batch normalisation on
    the input for the layer directly after it.
    Utilises a moving mean and variance calculation for use in test:
    while training, the predicted mean and variance of the entire population
    is adjusted at each batch, and then these predictions are used at test time.
    Layer based on this article:
    https://wiseodd.github.io/techblog/2016/07/04/batchnorm/
    &#34;&#34;&#34;

    def __init__(self, momentum: float = 0.9):
        &#34;&#34;&#34;
        :param momentum: rate of momentum applied when computing
        moving mean and variance
        &#34;&#34;&#34;
        super().__init__()
        # save appropriate values
        self.input = None
        self.momentum = momentum

        # initialise batchnorm params
        self.gamma = 1
        self.beta = 0
        # initialise values used for training
        self.grad_gamma = 0
        self.grad_beta = 0
        self.prev_grad_gamma = 0
        self.prev_grad_beta = 0
        # initialise moving mean and variance
        self.moving_mu = 0
        self.moving_var = 0
        # adam optimiser
        self.m_gamma = 0
        self.v_gamma = 0

        self.m_beta = 0
        self.v_beta = 0

    # we dont change the dimensions of the data in a batchnorm layer,
    # so in dim == out dim.
    def _add_prev_layer(self, prev_activation: Activation, prev_out_dim: int) -&gt; None:
        self.prev_activation = prev_activation
        self.in_dim = prev_out_dim
        self.out_dim = prev_out_dim

    def forward(self, input_batch: Array[float], mode: str = &#34;train&#34;) -&gt; Array[float]:
        if mode == &#34;train&#34;:
            # calc mu and var
            self.mu = np.mean(input_batch, axis=0)
            self.var = np.var(input_batch, axis=0)

            # batchnorm: compute moving mean and std for use in eval
            self.moving_mu = (
                self.momentum * self.moving_mu + (1 - self.momentum) * self.mu
            )
            self.moving_var = (
                self.momentum * self.moving_var + (1 - self.momentum) * self.var
            )
        else:
            # in test mode, we use the moving averages
            self.mu = self.moving_mu
            self.var = self.moving_var
        # normalise input
        self.X_norm = (input_batch - self.mu) / np.sqrt(self.var + 1e-8)
        # scale normalised input
        lin_output = self.X_norm * self.gamma + self.beta
        # we perform no activation on this layer: it is just batch norm-ing
        self.output = lin_output
        # cache input for backprop
        self.input = input_batch
        return self.output

    def backward(self, deltas: Array[float]) -&gt; Array[float]:
        # code based off https://wiseodd.github.io/techblog/2016/07/04/batchnorm/
        # but we have adapted it for our own purposes

        # cache previous gradients
        self.prev_grad_gamma = self.grad_gamma
        self.prev_grad_beta = self.grad_beta
        # calculate batchnorm gradients
        X_mu = self.input - self.mu
        std_inv = 1.0 / np.sqrt(self.var + 1e-8)

        dX_norm = deltas * self.gamma

        np.sum(dX_norm * X_mu, axis=0)
        dvar = np.sum(dX_norm * X_mu, axis=0) * -0.5 * (self.var + 1e-8) ** (-3 / 2)
        dmu = np.sum(dX_norm * -std_inv, axis=0) + dvar * np.mean(-2.0 * X_mu, axis=0)

        dX = (dX_norm * std_inv) + (dvar * 2 * X_mu / self.in_dim) + (dmu / self.in_dim)
        dgamma = np.sum(deltas * self.X_norm, axis=0)
        dbeta = np.sum(deltas, axis=0)
        # save batch norm gradients
        self.grad_gamma = dgamma
        self.grad_beta = dbeta

        if self.prev_activation is not None:
            deltas = dX * self.prev_activation.f_deriv(self.input)
        return deltas

    def update(self, lr: float, wd: float, m: float) -&gt; None:
        self.gamma -= (lr * (self.grad_gamma + m * self.prev_grad_gamma)) - (
            lr * wd * self.grad_gamma
        )
        self.beta -= (lr * (self.grad_beta + m * self.prev_grad_beta)) - (
            lr * wd * self.grad_beta
        )

    def update_adam(self, t: int, alpha: float, beta1: float, beta2: float) -&gt; None:
        # adam code based off the fourth tutorial of comp5329. Adapted for our purposes.
        self.m_gamma = beta1 * self.m_gamma + (1 - beta1) * self.grad_gamma
        self.v_gamma = beta2 * self.v_gamma + (1 - beta2) * (self.grad_gamma ** 2)
        m_hat = self.m_gamma / (1 - beta1 ** t)
        v_hat = self.v_gamma / (1 - beta2 ** t)
        self.gamma = self.gamma - alpha * (m_hat / (np.sqrt(v_hat) - 1e-8))

        self.m_beta = beta1 * self.m_beta + (1 - beta1) * self.grad_beta
        self.v_beta = beta2 * self.v_beta + (1 - beta2) * (self.grad_beta ** 2)
        m_hat = self.m_beta / (1 - beta1 ** t)
        v_hat = self.v_beta / (1 - beta2 ** t)
        self.beta = self.beta - alpha * (m_hat / (np.sqrt(v_hat) - 1e-8))</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="DenserFlow.layer.ActivationLayer"><code class="flex name class">
<span>class <span class="ident">ActivationLayer</span></span>
<span>(</span><span><small>ancestors:</small> <a title="DenserFlow.layer.Layer" href="#DenserFlow.layer.Layer">Layer</a>)</span>
</code></dt>
<dd>
<section class="desc"><p>A layer that just applies an activation function. This can be used if you
wish to apply an activation independent of a dense layer.</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">class ActivationLayer(Layer):
    &#34;&#34;&#34;
    A layer that just applies an activation function. This can be used if you
    wish to apply an activation independent of a dense layer.
    &#34;&#34;&#34;

    def __init__(self, activation: Activation):
        &#34;&#34;&#34;
        :param activation: The activation function to use.
        &#34;&#34;&#34;
        self.activation = activation

    # we dont change the dimensions of the data in an activation layer,
    # so in dim == out dim.
    def _add_prev_layer(self, prev_activation: Activation, prev_out_dim: int) -&gt; None:
        self.prev_activation = prev_activation
        self.in_dim = prev_out_dim
        self.out_dim = prev_out_dim

    def forward(self, input_batch: Array[float], mode: str = &#34;train&#34;) -&gt; Array[float]:
        output = np.apply_along_axis(self.activation.f, 1, input_batch)
        # save values for backward pass
        self.output = output
        self.input = input_batch
        return output

    def backward(self, deltas: Array[float]) -&gt; Array[float]:
        # prepare for next layer
        next_deltas = np.zeros((deltas.shape[0], self.in_dim))
        idx = 0
        for delta, sample in zip(deltas, self.input):
            if self.prev_activation is not None:
                next_deltas[idx] = delta * self.prev_activation.f_deriv(sample)
            idx += 1
        return next_deltas

    def update(self, lr: float, wd: float, m: float) -&gt; None:
        pass

    def update_adam(self, t: int, alpha: float, beta1: float, beta2: float) -&gt; None:
        pass</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="DenserFlow.layer.ActivationLayer.__init__"><code class="name flex">
<span>def <span class="ident">__init__</span></span>(<span>self, activation)</span>
</code></dt>
<dd>
<section class="desc"><p>:param activation: The activation function to use.</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def __init__(self, activation: Activation):
    &#34;&#34;&#34;
    :param activation: The activation function to use.
    &#34;&#34;&#34;
    self.activation = activation</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="DenserFlow.layer.Layer" href="#DenserFlow.layer.Layer">Layer</a></b></code>:
<ul class="hlist">
<li><code><a title="DenserFlow.layer.Layer.backward" href="#DenserFlow.layer.Layer.backward">backward</a></code></li>
<li><code><a title="DenserFlow.layer.Layer.forward" href="#DenserFlow.layer.Layer.forward">forward</a></code></li>
<li><code><a title="DenserFlow.layer.Layer.get_activation" href="#DenserFlow.layer.Layer.get_activation">get_activation</a></code></li>
<li><code><a title="DenserFlow.layer.Layer.update" href="#DenserFlow.layer.Layer.update">update</a></code></li>
<li><code><a title="DenserFlow.layer.Layer.update_adam" href="#DenserFlow.layer.Layer.update_adam">update_adam</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="DenserFlow.layer.BatchNormLayer"><code class="flex name class">
<span>class <span class="ident">BatchNormLayer</span></span>
<span>(</span><span><small>ancestors:</small> <a title="DenserFlow.layer.Layer" href="#DenserFlow.layer.Layer">Layer</a>)</span>
</code></dt>
<dd>
<section class="desc"><p>A Batch Normalisation layer. Performs batch normalisation on
the input for the layer directly after it.
Utilises a moving mean and variance calculation for use in test:
while training, the predicted mean and variance of the entire population
is adjusted at each batch, and then these predictions are used at test time.
Layer based on this article:
<a href="https://wiseodd.github.io/techblog/2016/07/04/batchnorm/">https://wiseodd.github.io/techblog/2016/07/04/batchnorm/</a></p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">class BatchNormLayer(Layer):
    &#34;&#34;&#34;
    A Batch Normalisation layer. Performs batch normalisation on
    the input for the layer directly after it.
    Utilises a moving mean and variance calculation for use in test:
    while training, the predicted mean and variance of the entire population
    is adjusted at each batch, and then these predictions are used at test time.
    Layer based on this article:
    https://wiseodd.github.io/techblog/2016/07/04/batchnorm/
    &#34;&#34;&#34;

    def __init__(self, momentum: float = 0.9):
        &#34;&#34;&#34;
        :param momentum: rate of momentum applied when computing
        moving mean and variance
        &#34;&#34;&#34;
        super().__init__()
        # save appropriate values
        self.input = None
        self.momentum = momentum

        # initialise batchnorm params
        self.gamma = 1
        self.beta = 0
        # initialise values used for training
        self.grad_gamma = 0
        self.grad_beta = 0
        self.prev_grad_gamma = 0
        self.prev_grad_beta = 0
        # initialise moving mean and variance
        self.moving_mu = 0
        self.moving_var = 0
        # adam optimiser
        self.m_gamma = 0
        self.v_gamma = 0

        self.m_beta = 0
        self.v_beta = 0

    # we dont change the dimensions of the data in a batchnorm layer,
    # so in dim == out dim.
    def _add_prev_layer(self, prev_activation: Activation, prev_out_dim: int) -&gt; None:
        self.prev_activation = prev_activation
        self.in_dim = prev_out_dim
        self.out_dim = prev_out_dim

    def forward(self, input_batch: Array[float], mode: str = &#34;train&#34;) -&gt; Array[float]:
        if mode == &#34;train&#34;:
            # calc mu and var
            self.mu = np.mean(input_batch, axis=0)
            self.var = np.var(input_batch, axis=0)

            # batchnorm: compute moving mean and std for use in eval
            self.moving_mu = (
                self.momentum * self.moving_mu + (1 - self.momentum) * self.mu
            )
            self.moving_var = (
                self.momentum * self.moving_var + (1 - self.momentum) * self.var
            )
        else:
            # in test mode, we use the moving averages
            self.mu = self.moving_mu
            self.var = self.moving_var
        # normalise input
        self.X_norm = (input_batch - self.mu) / np.sqrt(self.var + 1e-8)
        # scale normalised input
        lin_output = self.X_norm * self.gamma + self.beta
        # we perform no activation on this layer: it is just batch norm-ing
        self.output = lin_output
        # cache input for backprop
        self.input = input_batch
        return self.output

    def backward(self, deltas: Array[float]) -&gt; Array[float]:
        # code based off https://wiseodd.github.io/techblog/2016/07/04/batchnorm/
        # but we have adapted it for our own purposes

        # cache previous gradients
        self.prev_grad_gamma = self.grad_gamma
        self.prev_grad_beta = self.grad_beta
        # calculate batchnorm gradients
        X_mu = self.input - self.mu
        std_inv = 1.0 / np.sqrt(self.var + 1e-8)

        dX_norm = deltas * self.gamma

        np.sum(dX_norm * X_mu, axis=0)
        dvar = np.sum(dX_norm * X_mu, axis=0) * -0.5 * (self.var + 1e-8) ** (-3 / 2)
        dmu = np.sum(dX_norm * -std_inv, axis=0) + dvar * np.mean(-2.0 * X_mu, axis=0)

        dX = (dX_norm * std_inv) + (dvar * 2 * X_mu / self.in_dim) + (dmu / self.in_dim)
        dgamma = np.sum(deltas * self.X_norm, axis=0)
        dbeta = np.sum(deltas, axis=0)
        # save batch norm gradients
        self.grad_gamma = dgamma
        self.grad_beta = dbeta

        if self.prev_activation is not None:
            deltas = dX * self.prev_activation.f_deriv(self.input)
        return deltas

    def update(self, lr: float, wd: float, m: float) -&gt; None:
        self.gamma -= (lr * (self.grad_gamma + m * self.prev_grad_gamma)) - (
            lr * wd * self.grad_gamma
        )
        self.beta -= (lr * (self.grad_beta + m * self.prev_grad_beta)) - (
            lr * wd * self.grad_beta
        )

    def update_adam(self, t: int, alpha: float, beta1: float, beta2: float) -&gt; None:
        # adam code based off the fourth tutorial of comp5329. Adapted for our purposes.
        self.m_gamma = beta1 * self.m_gamma + (1 - beta1) * self.grad_gamma
        self.v_gamma = beta2 * self.v_gamma + (1 - beta2) * (self.grad_gamma ** 2)
        m_hat = self.m_gamma / (1 - beta1 ** t)
        v_hat = self.v_gamma / (1 - beta2 ** t)
        self.gamma = self.gamma - alpha * (m_hat / (np.sqrt(v_hat) - 1e-8))

        self.m_beta = beta1 * self.m_beta + (1 - beta1) * self.grad_beta
        self.v_beta = beta2 * self.v_beta + (1 - beta2) * (self.grad_beta ** 2)
        m_hat = self.m_beta / (1 - beta1 ** t)
        v_hat = self.v_beta / (1 - beta2 ** t)
        self.beta = self.beta - alpha * (m_hat / (np.sqrt(v_hat) - 1e-8))</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="DenserFlow.layer.BatchNormLayer.__init__"><code class="name flex">
<span>def <span class="ident">__init__</span></span>(<span>self, momentum=0.9)</span>
</code></dt>
<dd>
<section class="desc"><p>:param momentum: rate of momentum applied when computing
moving mean and variance</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def __init__(self, momentum: float = 0.9):
    &#34;&#34;&#34;
    :param momentum: rate of momentum applied when computing
    moving mean and variance
    &#34;&#34;&#34;
    super().__init__()
    # save appropriate values
    self.input = None
    self.momentum = momentum

    # initialise batchnorm params
    self.gamma = 1
    self.beta = 0
    # initialise values used for training
    self.grad_gamma = 0
    self.grad_beta = 0
    self.prev_grad_gamma = 0
    self.prev_grad_beta = 0
    # initialise moving mean and variance
    self.moving_mu = 0
    self.moving_var = 0
    # adam optimiser
    self.m_gamma = 0
    self.v_gamma = 0

    self.m_beta = 0
    self.v_beta = 0</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="DenserFlow.layer.Layer" href="#DenserFlow.layer.Layer">Layer</a></b></code>:
<ul class="hlist">
<li><code><a title="DenserFlow.layer.Layer.backward" href="#DenserFlow.layer.Layer.backward">backward</a></code></li>
<li><code><a title="DenserFlow.layer.Layer.forward" href="#DenserFlow.layer.Layer.forward">forward</a></code></li>
<li><code><a title="DenserFlow.layer.Layer.get_activation" href="#DenserFlow.layer.Layer.get_activation">get_activation</a></code></li>
<li><code><a title="DenserFlow.layer.Layer.update" href="#DenserFlow.layer.Layer.update">update</a></code></li>
<li><code><a title="DenserFlow.layer.Layer.update_adam" href="#DenserFlow.layer.Layer.update_adam">update_adam</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="DenserFlow.layer.DenseLayer"><code class="flex name class">
<span>class <span class="ident">DenseLayer</span></span>
<span>(</span><span><small>ancestors:</small> <a title="DenserFlow.layer.Layer" href="#DenserFlow.layer.Layer">Layer</a>)</span>
</code></dt>
<dd>
<section class="desc"><p>A fully connected layer. Weight matrix W should be of shape (n_in,n_out)
and the bias vector b should be of shape (n_out,).</p>
<p>Hidden unit activation is given by: f(dot(input,W) + b)</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">class DenseLayer(Layer):
    &#34;&#34;&#34;
     A fully connected layer. Weight matrix W should be of shape (n_in,n_out)
     and the bias vector b should be of shape (n_out,).

    Hidden unit activation is given by: f(dot(input,W) + b)

    &#34;&#34;&#34;

    def __init__(
        self,
        n_in: int,
        n_out: int,
        activation: Activation = linear(),
        W: Optional[Array[float]] = None,
        b: Optional[Array[float]] = None,
    ):
        &#34;&#34;&#34;
        :param n_in: dimensionality of input

        :param n_out: number of hidden units

        :param activation: Non linearity to be applied in the hidden layer

        :param W: Weight matrix to initialise weights with.
        If none, Xavier Initialisation is used.

        :param b: Bias matrix to initialise bias values with.
        If none, Xavier Initialisation is used.
        &#34;&#34;&#34;
        super().__init__(activation)
        # useful variable to store
        self.in_dim = n_in
        self.out_dim = n_out
        self.input = None

        # initialise weights and bias
        if W is None:
            # we use He initialisation if using relu
            if type(activation) is relu or type(activation) is leaky_relu:
                self.W = np.random.normal(
                    loc=0, scale=np.sqrt(2.0 / n_in), size=(n_in, n_out)
                )
            # else use xavier initialisation
            else:
                self.W = np.random.uniform(
                    low=-np.sqrt(6.0 / (n_in + n_out)),
                    high=np.sqrt(6.0 / (n_in + n_out)),
                    size=(n_in, n_out),
                )
            if type(activation) is logistic:
                self.W *= 4

        self.b = np.zeros((n_out))

        # initialise gradient matrices
        self.grad_W = np.zeros(self.W.shape)
        self.grad_b = np.zeros(self.b.shape)

        self.prev_grad_W = np.zeros(self.W.shape)
        self.prev_grad_b = np.zeros(self.b.shape)

        self.m_W = np.zeros(self.W.shape)
        self.v_W = np.zeros(self.W.shape)

        self.m_b = np.zeros(self.b.shape)
        self.v_b = np.zeros(self.b.shape)

    # utility function for forward pass below
    def _apply_weights(self, input_row):
        return np.dot(input_row, self.W)

    def forward(self, input_batch: Array[float], mode: str = &#34;train&#34;) -&gt; Array[float]:
        output = np.zeros((input_batch.shape[0], self.out_dim))
        # iterate over each sample in the minibatch
        lin_output = np.apply_along_axis(self._apply_weights, 1, input_batch) + self.b
        output = np.apply_along_axis(self.activation.f, 1, lin_output)
        # save values for backward pass
        self.output = output
        self.input = input_batch
        return self.output

    def backward(self, deltas: Array[float]) -&gt; Array[float]:
        # save previous gradients
        self.prev_grad_W = self.grad_W
        self.prev_grad_b = self.grad_b
        # accumulate gradients over the minibatch
        self.grad_W = np.zeros_like(self.W)
        self.grad_b = np.zeros_like(self.b)
        # prepare for next layer
        next_deltas = np.zeros((deltas.shape[0], self.in_dim))
        idx = 0
        self.grad_b += np.sum(deltas)
        for delta, sample in zip(deltas, self.input):
            self.grad_W += np.atleast_2d(sample).T.dot(np.atleast_2d(delta))
            if self.prev_activation is not None:
                next_deltas[idx] = delta.dot(self.W.T) * self.prev_activation.f_deriv(
                    sample
                )
            idx += 1
        return next_deltas

    def update_adam(self, t: int, alpha: float, beta1: float, beta2: float) -&gt; None:
        # adam code based off the fourth tutorial of comp5329. Adapted for our purposes.
        self.m_W = beta1 * self.m_W + (1 - beta1) * self.grad_W
        self.v_W = beta2 * self.v_W + (1 - beta2) * (self.grad_W ** 2)
        m_hat = self.m_W / (1 - beta1 ** t)
        v_hat = self.v_W / (1 - beta2 ** t)
        self.W = self.W - alpha * (m_hat / (np.sqrt(v_hat) - 1e-8))

        self.m_b = beta1 * self.m_b + (1 - beta1) * self.grad_b
        self.v_b = beta2 * self.v_b + (1 - beta2) * (self.grad_b ** 2)
        m_hat = self.m_b / (1 - beta1 ** t)
        v_hat = self.v_b / (1 - beta2 ** t)
        self.b = self.b - alpha * (m_hat / (np.sqrt(v_hat) - 1e-8))

    def update(self, lr: float, wd: float, m: float) -&gt; None:
        self.W -= (lr * (self.grad_W + m * self.prev_grad_W)) - (lr * wd * self.grad_W)
        self.b -= (lr * (self.grad_b + m * self.prev_grad_b)) - (lr * wd * self.grad_b)</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="DenserFlow.layer.DenseLayer.__init__"><code class="name flex">
<span>def <span class="ident">__init__</span></span>(<span>self, n_in, n_out, activation=&lt;DenserFlow.activation.linear object at 0x7f9e157a69e8&gt;, W=None, b=None)</span>
</code></dt>
<dd>
<section class="desc"><p>:param n_in: dimensionality of input</p>
<p>:param n_out: number of hidden units</p>
<p>:param activation: Non linearity to be applied in the hidden layer</p>
<p>:param W: Weight matrix to initialise weights with.
If none, Xavier Initialisation is used.</p>
<p>:param b: Bias matrix to initialise bias values with.
If none, Xavier Initialisation is used.</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def __init__(
    self,
    n_in: int,
    n_out: int,
    activation: Activation = linear(),
    W: Optional[Array[float]] = None,
    b: Optional[Array[float]] = None,
):
    &#34;&#34;&#34;
    :param n_in: dimensionality of input

    :param n_out: number of hidden units

    :param activation: Non linearity to be applied in the hidden layer

    :param W: Weight matrix to initialise weights with.
    If none, Xavier Initialisation is used.

    :param b: Bias matrix to initialise bias values with.
    If none, Xavier Initialisation is used.
    &#34;&#34;&#34;
    super().__init__(activation)
    # useful variable to store
    self.in_dim = n_in
    self.out_dim = n_out
    self.input = None

    # initialise weights and bias
    if W is None:
        # we use He initialisation if using relu
        if type(activation) is relu or type(activation) is leaky_relu:
            self.W = np.random.normal(
                loc=0, scale=np.sqrt(2.0 / n_in), size=(n_in, n_out)
            )
        # else use xavier initialisation
        else:
            self.W = np.random.uniform(
                low=-np.sqrt(6.0 / (n_in + n_out)),
                high=np.sqrt(6.0 / (n_in + n_out)),
                size=(n_in, n_out),
            )
        if type(activation) is logistic:
            self.W *= 4

    self.b = np.zeros((n_out))

    # initialise gradient matrices
    self.grad_W = np.zeros(self.W.shape)
    self.grad_b = np.zeros(self.b.shape)

    self.prev_grad_W = np.zeros(self.W.shape)
    self.prev_grad_b = np.zeros(self.b.shape)

    self.m_W = np.zeros(self.W.shape)
    self.v_W = np.zeros(self.W.shape)

    self.m_b = np.zeros(self.b.shape)
    self.v_b = np.zeros(self.b.shape)</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="DenserFlow.layer.Layer" href="#DenserFlow.layer.Layer">Layer</a></b></code>:
<ul class="hlist">
<li><code><a title="DenserFlow.layer.Layer.backward" href="#DenserFlow.layer.Layer.backward">backward</a></code></li>
<li><code><a title="DenserFlow.layer.Layer.forward" href="#DenserFlow.layer.Layer.forward">forward</a></code></li>
<li><code><a title="DenserFlow.layer.Layer.get_activation" href="#DenserFlow.layer.Layer.get_activation">get_activation</a></code></li>
<li><code><a title="DenserFlow.layer.Layer.update" href="#DenserFlow.layer.Layer.update">update</a></code></li>
<li><code><a title="DenserFlow.layer.Layer.update_adam" href="#DenserFlow.layer.Layer.update_adam">update_adam</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="DenserFlow.layer.Dropout"><code class="flex name class">
<span>class <span class="ident">Dropout</span></span>
<span>(</span><span><small>ancestors:</small> <a title="DenserFlow.layer.Layer" href="#DenserFlow.layer.Layer">Layer</a>)</span>
</code></dt>
<dd>
<section class="desc"><p>A Dropout layer. This randomly drops nodes in the previous
layer at a given probability rate.</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">class Dropout(Layer):
    &#34;&#34;&#34;
    A Dropout layer. This randomly drops nodes in the previous
    layer at a given probability rate.
    &#34;&#34;&#34;

    def __init__(self, dropout_rate: float):
        &#34;&#34;&#34;
        :param dropout_rate: The rate at which units should be dropped.
        Must be between 0 and 1.
        &#34;&#34;&#34;
        super().__init__()
        self.keep_rate = 1 - dropout_rate

    # we dont change the dimensions of the data in a dropout layer,
    # so in dim == out dim.
    def _add_prev_layer(self, prev_activation: Activation, prev_out_dim: int) -&gt; None:
        self.prev_activation = prev_activation
        self.in_dim = prev_out_dim
        self.out_dim = prev_out_dim

    def forward(self, input_batch: Array[float], mode: str = &#34;train&#34;) -&gt; Array[float]:
        self.dropout_matrix = (
            np.random.binomial(1, self.keep_rate, size=input_batch.shape)
            / self.keep_rate
        )
        return np.reshape(input_batch * self.dropout_matrix, input_batch.shape)

    def backward(self, deltas: Array[float]) -&gt; Array[float]:
        return deltas * self.dropout_matrix

    # nothing to learn, so nothing happens in update
    def update(self, lr: float, wd: float, m: float) -&gt; None:
        pass

    def update_adam(self, t: int, alpha: float, beta1: float, beta2: float) -&gt; None:
        pass</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="DenserFlow.layer.Dropout.__init__"><code class="name flex">
<span>def <span class="ident">__init__</span></span>(<span>self, dropout_rate)</span>
</code></dt>
<dd>
<section class="desc"><p>:param dropout_rate: The rate at which units should be dropped.
Must be between 0 and 1.</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def __init__(self, dropout_rate: float):
    &#34;&#34;&#34;
    :param dropout_rate: The rate at which units should be dropped.
    Must be between 0 and 1.
    &#34;&#34;&#34;
    super().__init__()
    self.keep_rate = 1 - dropout_rate</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="DenserFlow.layer.Layer" href="#DenserFlow.layer.Layer">Layer</a></b></code>:
<ul class="hlist">
<li><code><a title="DenserFlow.layer.Layer.backward" href="#DenserFlow.layer.Layer.backward">backward</a></code></li>
<li><code><a title="DenserFlow.layer.Layer.forward" href="#DenserFlow.layer.Layer.forward">forward</a></code></li>
<li><code><a title="DenserFlow.layer.Layer.get_activation" href="#DenserFlow.layer.Layer.get_activation">get_activation</a></code></li>
<li><code><a title="DenserFlow.layer.Layer.update" href="#DenserFlow.layer.Layer.update">update</a></code></li>
<li><code><a title="DenserFlow.layer.Layer.update_adam" href="#DenserFlow.layer.Layer.update_adam">update_adam</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="DenserFlow.layer.Layer"><code class="flex name class">
<span>class <span class="ident">Layer</span></span>
</code></dt>
<dd>
<section class="desc"><p>The parent class for all layer classes.</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">class Layer(object):
    &#34;&#34;&#34;
    The parent class for all layer classes.
    &#34;&#34;&#34;

    def __init__(self, activation: Activation = linear()):
        self.prev_activation = None
        self.activation = activation

    def _add_prev_layer(self, prev_activation: Activation, prev_out_dim: int) -&gt; None:
        &#34;&#34;&#34;
        Utility function used when creating a model. For our
        implementation of backpropagation, the activation function
        of the previous layer in the network must be known. This
        should not be called by the end user.
        &#34;&#34;&#34;
        self.prev_activation = prev_activation
        self.in_dim = prev_out_dim

    def get_activation(self) -&gt; Activation:
        &#34;&#34;&#34;
        Returns activation function for this layer.
        &#34;&#34;&#34;
        return self.activation

    def forward(self, input_batch: Array[float], mode: str = &#34;train&#34;) -&gt; Array[float]:
        &#34;&#34;&#34;
        Performs a forward pass given an input batch.
        :param input_batch: the input to the layer
        :param mode: whether we are training or testing the layer at this point.
        &#34;&#34;&#34;
        raise AttributeError(&#34;Unimplemented layer function&#34;)

    def backward(self, deltas: Array[float]) -&gt; Array[float]:
        &#34;&#34;&#34;
        Performs a backward pass given a delta.
        :param deltas: the delta passed through from the previous layer -
        the derivative of the error function with respect to the
        net of the current layer.
        &#34;&#34;&#34;
        raise AttributeError(&#34;Unimplemented layer function&#34;)

    def update(self, lr: float, wd: float, m: float) -&gt; None:
        &#34;&#34;&#34;
        Update the weights of this layer.
        :param lr: the learning rate of the model.
        :param wd: the weight decay rate of the model.
        :param m: the momentum rate of the model.
        &#34;&#34;&#34;
        raise AttributeError(&#34;Unimplemented layer functions&#34;)

    def update_adam(self, t: int, alpha: float, beta1: float, beta2: float) -&gt; None:
        &#34;&#34;&#34;
        Update the weights of this layer using the adam algorithm.
        :param t: the current timestep of the model (usually the current epoch number)
        :param alpha: the learning rate of the model.
        :param beta1: the decay rate for the first moment.
        :param beta2: the decay rate for the second moment.
        &#34;&#34;&#34;
        raise AttributeError(&#34;Unimplemented layer functions&#34;)</code></pre>
</details>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="DenserFlow.layer.DenseLayer" href="#DenserFlow.layer.DenseLayer">DenseLayer</a></li>
<li><a title="DenserFlow.layer.ActivationLayer" href="#DenserFlow.layer.ActivationLayer">ActivationLayer</a></li>
<li><a title="DenserFlow.layer.Dropout" href="#DenserFlow.layer.Dropout">Dropout</a></li>
<li><a title="DenserFlow.layer.BatchNormLayer" href="#DenserFlow.layer.BatchNormLayer">BatchNormLayer</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="DenserFlow.layer.Layer.__init__"><code class="name flex">
<span>def <span class="ident">__init__</span></span>(<span>self, activation=&lt;DenserFlow.activation.linear object at 0x7f9e0a5bcfd0&gt;)</span>
</code></dt>
<dd>
<section class="desc"><p>Initialize self.
See help(type(self)) for accurate signature.</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def __init__(self, activation: Activation = linear()):
    self.prev_activation = None
    self.activation = activation</code></pre>
</details>
</dd>
<dt id="DenserFlow.layer.Layer.backward"><code class="name flex">
<span>def <span class="ident">backward</span></span>(<span>self, deltas)</span>
</code></dt>
<dd>
<section class="desc"><p>Performs a backward pass given a delta.
:param deltas: the delta passed through from the previous layer -
the derivative of the error function with respect to the
net of the current layer.</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def backward(self, deltas: Array[float]) -&gt; Array[float]:
    &#34;&#34;&#34;
    Performs a backward pass given a delta.
    :param deltas: the delta passed through from the previous layer -
    the derivative of the error function with respect to the
    net of the current layer.
    &#34;&#34;&#34;
    raise AttributeError(&#34;Unimplemented layer function&#34;)</code></pre>
</details>
</dd>
<dt id="DenserFlow.layer.Layer.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, input_batch, mode=&#39;train&#39;)</span>
</code></dt>
<dd>
<section class="desc"><p>Performs a forward pass given an input batch.<br>
:param input_batch: the input to the layer<br>
:param mode: whether we are training or testing the layer at this point.</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def forward(self, input_batch: Array[float], mode: str = &#34;train&#34;) -&gt; Array[float]:
    &#34;&#34;&#34;
    Performs a forward pass given an input batch.
    :param input_batch: the input to the layer
    :param mode: whether we are training or testing the layer at this point.
    &#34;&#34;&#34;
    raise AttributeError(&#34;Unimplemented layer function&#34;)</code></pre>
</details>
</dd>
<dt id="DenserFlow.layer.Layer.get_activation"><code class="name flex">
<span>def <span class="ident">get_activation</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Returns activation function for this layer.</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def get_activation(self) -&gt; Activation:
    &#34;&#34;&#34;
    Returns activation function for this layer.
    &#34;&#34;&#34;
    return self.activation</code></pre>
</details>
</dd>
<dt id="DenserFlow.layer.Layer.update"><code class="name flex">
<span>def <span class="ident">update</span></span>(<span>self, lr, wd, m)</span>
</code></dt>
<dd>
<section class="desc"><p>Update the weights of this layer.<br>
:param lr: the learning rate of the model.<br>
:param wd: the weight decay rate of the model.<br>
:param m: the momentum rate of the model.</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def update(self, lr: float, wd: float, m: float) -&gt; None:
    &#34;&#34;&#34;
    Update the weights of this layer.
    :param lr: the learning rate of the model.
    :param wd: the weight decay rate of the model.
    :param m: the momentum rate of the model.
    &#34;&#34;&#34;
    raise AttributeError(&#34;Unimplemented layer functions&#34;)</code></pre>
</details>
</dd>
<dt id="DenserFlow.layer.Layer.update_adam"><code class="name flex">
<span>def <span class="ident">update_adam</span></span>(<span>self, t, alpha, beta1, beta2)</span>
</code></dt>
<dd>
<section class="desc"><p>Update the weights of this layer using the adam algorithm.<br>
:param t: the current timestep of the model (usually the current epoch number)<br>
:param alpha: the learning rate of the model.<br>
:param beta1: the decay rate for the first moment.<br>
:param beta2: the decay rate for the second moment.</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def update_adam(self, t: int, alpha: float, beta1: float, beta2: float) -&gt; None:
    &#34;&#34;&#34;
    Update the weights of this layer using the adam algorithm.
    :param t: the current timestep of the model (usually the current epoch number)
    :param alpha: the learning rate of the model.
    :param beta1: the decay rate for the first moment.
    :param beta2: the decay rate for the second moment.
    &#34;&#34;&#34;
    raise AttributeError(&#34;Unimplemented layer functions&#34;)</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="DenserFlow" href="index.html">DenserFlow</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="DenserFlow.layer.ActivationLayer" href="#DenserFlow.layer.ActivationLayer">ActivationLayer</a></code></h4>
<ul class="">
<li><code><a title="DenserFlow.layer.ActivationLayer.__init__" href="#DenserFlow.layer.ActivationLayer.__init__">__init__</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="DenserFlow.layer.BatchNormLayer" href="#DenserFlow.layer.BatchNormLayer">BatchNormLayer</a></code></h4>
<ul class="">
<li><code><a title="DenserFlow.layer.BatchNormLayer.__init__" href="#DenserFlow.layer.BatchNormLayer.__init__">__init__</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="DenserFlow.layer.DenseLayer" href="#DenserFlow.layer.DenseLayer">DenseLayer</a></code></h4>
<ul class="">
<li><code><a title="DenserFlow.layer.DenseLayer.__init__" href="#DenserFlow.layer.DenseLayer.__init__">__init__</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="DenserFlow.layer.Dropout" href="#DenserFlow.layer.Dropout">Dropout</a></code></h4>
<ul class="">
<li><code><a title="DenserFlow.layer.Dropout.__init__" href="#DenserFlow.layer.Dropout.__init__">__init__</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="DenserFlow.layer.Layer" href="#DenserFlow.layer.Layer">Layer</a></code></h4>
<ul class="two-column">
<li><code><a title="DenserFlow.layer.Layer.__init__" href="#DenserFlow.layer.Layer.__init__">__init__</a></code></li>
<li><code><a title="DenserFlow.layer.Layer.backward" href="#DenserFlow.layer.Layer.backward">backward</a></code></li>
<li><code><a title="DenserFlow.layer.Layer.forward" href="#DenserFlow.layer.Layer.forward">forward</a></code></li>
<li><code><a title="DenserFlow.layer.Layer.get_activation" href="#DenserFlow.layer.Layer.get_activation">get_activation</a></code></li>
<li><code><a title="DenserFlow.layer.Layer.update" href="#DenserFlow.layer.Layer.update">update</a></code></li>
<li><code><a title="DenserFlow.layer.Layer.update_adam" href="#DenserFlow.layer.Layer.update_adam">update_adam</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.5.4</a>.</p>
</footer>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad()</script>
</body>
</html>