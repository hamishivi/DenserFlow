<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.5.4" />
<title>DenserFlow.model API documentation</title>
<meta name="description" content="Defines a model in Denserflow. A model is a full neural network,
and is comprised of a set of layers with associated loss function
and activation â€¦" />
<link href='https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css' rel='stylesheet'>
<link href='https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/8.0.0/sanitize.min.css' rel='stylesheet'>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">
<style>.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{font-weight:bold}#index h4 + ul{margin-bottom:.6em}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.name small{font-weight:normal}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase;cursor:pointer}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title"><code>DenserFlow.model</code> module</h1>
</header>
<section id="section-intro">
<p>Defines a model in Denserflow. A model is a full neural network,
and is comprised of a set of layers with associated loss function
and activation functions. Learning happens online.</p>
<p>Unlike other modules, there is only one model class, since the
user composes their own models.</p>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">&#34;&#34;&#34;
Defines a model in Denserflow. A model is a full neural network,
and is comprised of a set of layers with associated loss function
and activation functions. Learning happens online.

Unlike other modules, there is only one model class, since the
user composes their own models.
&#34;&#34;&#34;
import logging
import random
from typing import List, Callable

import numpy as np
from nptyping import Array

from .activation import softmax
from .layer import Layer
from .error import Loss, CrossEntropyWithSoftmax

logger = logging.getLogger(&#34;DenserFlow.Model&#34;)


class Model:
    &#34;&#34;&#34;
    Represents a neural network model.
    &#34;&#34;&#34;

    def __init__(self, loss_func: Loss):
        &#34;&#34;&#34;
        :param loss_func: The loss funcation to be used for this model.
        &#34;&#34;&#34;
        # initialize layers
        self.layers = []
        self.params = []
        self.error_func = loss_func

    def add_layer(self, layer: Layer) -&gt; None:
        &#34;&#34;&#34;
        Add a layer to the model.
        :param layer: The layer to add.
        &#34;&#34;&#34;
        if self.layers:
            layer._add_prev_layer(
                self.layers[-1].get_activation(), self.layers[-1].out_dim
            )
        self.layers.append(layer)

    def forward(self, input_batch: Array[float], mode: str = &#34;train&#34;) -&gt; Array[float]:
        &#34;&#34;&#34;
        Perform a forward pass on the network for training.
        :param input_batch: The minibatched input. Must have shape (batch_size,)
        :param mode: Whether this is being used in train or test mode.
        &#34;&#34;&#34;
        for layer in self.layers:
            output = layer.forward(input_batch, mode)
            input_batch = output
        return output

    def backward(self, delta: Array[float]) -&gt; None:
        &#34;&#34;&#34;
        Perform a backward pass on the network, given the delta of the error function.
        :param delta: The derivative of the error function of network with
        respect to the net of the output layer.
        &#34;&#34;&#34;
        delta = self.layers[-1].backward(delta)
        for layer in reversed(self.layers[:-1]):
            delta = layer.backward(delta)

    def update(self, lr: float, wd: float = 0, m: float = 0) -&gt; None:
        &#34;&#34;&#34;
        Updates the weights in the network, using gradients calculated
        in a backward pass.
        :param lr: learning rate to use
        :param wd: weight decay rate to use
        :param m: momentum rate to use
        &#34;&#34;&#34;
        for layer in self.layers:
            layer.update(lr, wd, m)

    def update_adam(
        self, t: float, alpha: float = 0.001, beta1: float = 0.9, beta2: float = 0.999
    ) -&gt; None:
        &#34;&#34;&#34;
        Updates the weights in the network, using gradients calculated
        in a backward pass.
        :param lr: learning rate to use
        :param wd: weight decay rate to use
        :param m: momentum rate to use
        &#34;&#34;&#34;
        for layer in self.layers:
            layer.update_adam(t, alpha, beta1, beta2)

    def make_batch(
        self,
        x: Array[float],
        y: Array[float],
        minibatch_size: int,
        shuffle: bool = True,
    ) -&gt; List[Array[float]]:
        &#34;&#34;&#34;
        Makes minibatches from given data, and optionally shuffles them.

        :param x: array of sample inputs
        :param y: array of sample labels
        :param minibatch_size: size of minibatches
        :param shuffle: Whether to shuffle the minibatches or not.
        &#34;&#34;&#34;
        minibatches = []
        for idx in range(len(x) // minibatch_size):
            x_mini = []
            y_mini = []
            for i in range(idx, idx + minibatch_size):
                # Get pair of (X, y) of the current minibatch/chunk
                x_mini.append(x[i])
                y_mini.append(y[i])
            minibatches.append((np.array(x_mini), np.array(y_mini)))
        if shuffle:
            random.shuffle(minibatches)
        return minibatches

    def SGD(
        self,
        x: Array[float],
        y: Array[float],
        learning_rate: float = 0.01,
        weight_decay: float = 0.0,
        momentum: float = 0.0,
        minibatch_size: int = 1,
        epochs: int = 100,
        adam: bool = False,
        beta1: float = 0.9,
        beta2: float = 0.999,
        callback: Callable[[&#34;Model&#34;, int], float] = None,
    ) -&gt; Array[float]:
        &#34;&#34;&#34;
        Online learning with stochastic gradient descent. Returns an
        array of loss values at each epoch. If adam is true, then adam
        is used to optimise the network. A callback may also be passed in.
        At the end of each epoch, the callback will be called with the epoch
        number and the model object.

        :param x: Input data or features
        :param y: Input targets
        :param learning_rate: the learning rate used
        :param weight_decay: rate of weight decay used
        :param momentum: rate of momentum used
        :param epochs: number of times the dataset is presented to the
        network for learning
        :param: adam: If true, adam is used to optimise the network.
        :param beta1: Beta 1 value used in Adam optimisation.
        :param beta2: Beta 2 value used in Adam optimisation.
        :param callback: a callback function, provided the model itself.
        &#34;&#34;&#34;
        x = np.array(x)
        y = np.array(y)
        loss_vals = np.zeros(epochs)

        # train!
        for k in range(epochs):
            theta = None
            loss = np.zeros(x.shape[0])

            minibatches = self.make_batch(x, y, minibatch_size)

            for x_mini, y_mini in minibatches:
                # ensure we have our minibatch sizes
                x_mini.reshape((minibatch_size, -1))
                y_mini.reshape((minibatch_size, -1))
                # make our predictions
                y_hat = self.forward(x_mini)
                # calculate loss and deltas
                loss, theta = self.error_func.calc_loss(
                    y_mini, y_hat, self.layers[-1].get_activation().f_deriv
                )
                # calculate our weights and then update
                self.backward(theta)

                if adam:
                    self.update_adam(k + 1, learning_rate, beta1, beta2)
                else:
                    self.update(learning_rate, weight_decay, momentum)

                loss_vals[k] = np.mean(loss)
            logger.info(&#34;epoch &#34; + str(k + 1) + &#34; loss: &#34; + str(loss_vals[k]))
            # run callback if we have it
            if callback:
                callback(self, k)

        return loss_vals

    def predict(self, x: Array[float]) -&gt; Array[float]:
        &#34;&#34;&#34;
        Get the predictions of the model on a set of inputs.
        :param x: batch of inputs to feed into the model. Must have shape (batch_size,)
        &#34;&#34;&#34;
        output = np.zeros((x.shape[0], self.layers[-1].out_dim))
        # for each sample
        for i in np.arange(x.shape[0]):
            output[i] = self.forward(x[i, :].reshape(1, -1), mode=&#34;test&#34;)
            # special case - we need to apply softmax without the loss function
            if type(self.error_func) is CrossEntropyWithSoftmax:
                output[i] = softmax().f(output[i])
        return output</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="DenserFlow.model.Model"><code class="flex name class">
<span>class <span class="ident">Model</span></span>
</code></dt>
<dd>
<section class="desc"><p>Represents a neural network model.</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">class Model:
    &#34;&#34;&#34;
    Represents a neural network model.
    &#34;&#34;&#34;

    def __init__(self, loss_func: Loss):
        &#34;&#34;&#34;
        :param loss_func: The loss funcation to be used for this model.
        &#34;&#34;&#34;
        # initialize layers
        self.layers = []
        self.params = []
        self.error_func = loss_func

    def add_layer(self, layer: Layer) -&gt; None:
        &#34;&#34;&#34;
        Add a layer to the model.
        :param layer: The layer to add.
        &#34;&#34;&#34;
        if self.layers:
            layer._add_prev_layer(
                self.layers[-1].get_activation(), self.layers[-1].out_dim
            )
        self.layers.append(layer)

    def forward(self, input_batch: Array[float], mode: str = &#34;train&#34;) -&gt; Array[float]:
        &#34;&#34;&#34;
        Perform a forward pass on the network for training.
        :param input_batch: The minibatched input. Must have shape (batch_size,)
        :param mode: Whether this is being used in train or test mode.
        &#34;&#34;&#34;
        for layer in self.layers:
            output = layer.forward(input_batch, mode)
            input_batch = output
        return output

    def backward(self, delta: Array[float]) -&gt; None:
        &#34;&#34;&#34;
        Perform a backward pass on the network, given the delta of the error function.
        :param delta: The derivative of the error function of network with
        respect to the net of the output layer.
        &#34;&#34;&#34;
        delta = self.layers[-1].backward(delta)
        for layer in reversed(self.layers[:-1]):
            delta = layer.backward(delta)

    def update(self, lr: float, wd: float = 0, m: float = 0) -&gt; None:
        &#34;&#34;&#34;
        Updates the weights in the network, using gradients calculated
        in a backward pass.
        :param lr: learning rate to use
        :param wd: weight decay rate to use
        :param m: momentum rate to use
        &#34;&#34;&#34;
        for layer in self.layers:
            layer.update(lr, wd, m)

    def update_adam(
        self, t: float, alpha: float = 0.001, beta1: float = 0.9, beta2: float = 0.999
    ) -&gt; None:
        &#34;&#34;&#34;
        Updates the weights in the network, using gradients calculated
        in a backward pass.
        :param lr: learning rate to use
        :param wd: weight decay rate to use
        :param m: momentum rate to use
        &#34;&#34;&#34;
        for layer in self.layers:
            layer.update_adam(t, alpha, beta1, beta2)

    def make_batch(
        self,
        x: Array[float],
        y: Array[float],
        minibatch_size: int,
        shuffle: bool = True,
    ) -&gt; List[Array[float]]:
        &#34;&#34;&#34;
        Makes minibatches from given data, and optionally shuffles them.

        :param x: array of sample inputs
        :param y: array of sample labels
        :param minibatch_size: size of minibatches
        :param shuffle: Whether to shuffle the minibatches or not.
        &#34;&#34;&#34;
        minibatches = []
        for idx in range(len(x) // minibatch_size):
            x_mini = []
            y_mini = []
            for i in range(idx, idx + minibatch_size):
                # Get pair of (X, y) of the current minibatch/chunk
                x_mini.append(x[i])
                y_mini.append(y[i])
            minibatches.append((np.array(x_mini), np.array(y_mini)))
        if shuffle:
            random.shuffle(minibatches)
        return minibatches

    def SGD(
        self,
        x: Array[float],
        y: Array[float],
        learning_rate: float = 0.01,
        weight_decay: float = 0.0,
        momentum: float = 0.0,
        minibatch_size: int = 1,
        epochs: int = 100,
        adam: bool = False,
        beta1: float = 0.9,
        beta2: float = 0.999,
        callback: Callable[[&#34;Model&#34;, int], float] = None,
    ) -&gt; Array[float]:
        &#34;&#34;&#34;
        Online learning with stochastic gradient descent. Returns an
        array of loss values at each epoch. If adam is true, then adam
        is used to optimise the network. A callback may also be passed in.
        At the end of each epoch, the callback will be called with the epoch
        number and the model object.

        :param x: Input data or features
        :param y: Input targets
        :param learning_rate: the learning rate used
        :param weight_decay: rate of weight decay used
        :param momentum: rate of momentum used
        :param epochs: number of times the dataset is presented to the
        network for learning
        :param: adam: If true, adam is used to optimise the network.
        :param beta1: Beta 1 value used in Adam optimisation.
        :param beta2: Beta 2 value used in Adam optimisation.
        :param callback: a callback function, provided the model itself.
        &#34;&#34;&#34;
        x = np.array(x)
        y = np.array(y)
        loss_vals = np.zeros(epochs)

        # train!
        for k in range(epochs):
            theta = None
            loss = np.zeros(x.shape[0])

            minibatches = self.make_batch(x, y, minibatch_size)

            for x_mini, y_mini in minibatches:
                # ensure we have our minibatch sizes
                x_mini.reshape((minibatch_size, -1))
                y_mini.reshape((minibatch_size, -1))
                # make our predictions
                y_hat = self.forward(x_mini)
                # calculate loss and deltas
                loss, theta = self.error_func.calc_loss(
                    y_mini, y_hat, self.layers[-1].get_activation().f_deriv
                )
                # calculate our weights and then update
                self.backward(theta)

                if adam:
                    self.update_adam(k + 1, learning_rate, beta1, beta2)
                else:
                    self.update(learning_rate, weight_decay, momentum)

                loss_vals[k] = np.mean(loss)
            logger.info(&#34;epoch &#34; + str(k + 1) + &#34; loss: &#34; + str(loss_vals[k]))
            # run callback if we have it
            if callback:
                callback(self, k)

        return loss_vals

    def predict(self, x: Array[float]) -&gt; Array[float]:
        &#34;&#34;&#34;
        Get the predictions of the model on a set of inputs.
        :param x: batch of inputs to feed into the model. Must have shape (batch_size,)
        &#34;&#34;&#34;
        output = np.zeros((x.shape[0], self.layers[-1].out_dim))
        # for each sample
        for i in np.arange(x.shape[0]):
            output[i] = self.forward(x[i, :].reshape(1, -1), mode=&#34;test&#34;)
            # special case - we need to apply softmax without the loss function
            if type(self.error_func) is CrossEntropyWithSoftmax:
                output[i] = softmax().f(output[i])
        return output</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="DenserFlow.model.Model.SGD"><code class="name flex">
<span>def <span class="ident">SGD</span></span>(<span>self, x, y, learning_rate=0.01, weight_decay=0.0, momentum=0.0, minibatch_size=1, epochs=100, adam=False, beta1=0.9, beta2=0.999, callback=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Online learning with stochastic gradient descent. Returns an
array of loss values at each epoch. If adam is true, then adam
is used to optimise the network. A callback may also be passed in.
At the end of each epoch, the callback will be called with the epoch
number and the model object.</p>
<p>:param x: Input data or features
:param y: Input targets
:param learning_rate: the learning rate used
:param weight_decay: rate of weight decay used
:param momentum: rate of momentum used
:param epochs: number of times the dataset is presented to the
network for learning
:param: adam: If true, adam is used to optimise the network.
:param beta1: Beta 1 value used in Adam optimisation.
:param beta2: Beta 2 value used in Adam optimisation.
:param callback: a callback function, provided the model itself.</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def SGD(
    self,
    x: Array[float],
    y: Array[float],
    learning_rate: float = 0.01,
    weight_decay: float = 0.0,
    momentum: float = 0.0,
    minibatch_size: int = 1,
    epochs: int = 100,
    adam: bool = False,
    beta1: float = 0.9,
    beta2: float = 0.999,
    callback: Callable[[&#34;Model&#34;, int], float] = None,
) -&gt; Array[float]:
    &#34;&#34;&#34;
    Online learning with stochastic gradient descent. Returns an
    array of loss values at each epoch. If adam is true, then adam
    is used to optimise the network. A callback may also be passed in.
    At the end of each epoch, the callback will be called with the epoch
    number and the model object.

    :param x: Input data or features
    :param y: Input targets
    :param learning_rate: the learning rate used
    :param weight_decay: rate of weight decay used
    :param momentum: rate of momentum used
    :param epochs: number of times the dataset is presented to the
    network for learning
    :param: adam: If true, adam is used to optimise the network.
    :param beta1: Beta 1 value used in Adam optimisation.
    :param beta2: Beta 2 value used in Adam optimisation.
    :param callback: a callback function, provided the model itself.
    &#34;&#34;&#34;
    x = np.array(x)
    y = np.array(y)
    loss_vals = np.zeros(epochs)

    # train!
    for k in range(epochs):
        theta = None
        loss = np.zeros(x.shape[0])

        minibatches = self.make_batch(x, y, minibatch_size)

        for x_mini, y_mini in minibatches:
            # ensure we have our minibatch sizes
            x_mini.reshape((minibatch_size, -1))
            y_mini.reshape((minibatch_size, -1))
            # make our predictions
            y_hat = self.forward(x_mini)
            # calculate loss and deltas
            loss, theta = self.error_func.calc_loss(
                y_mini, y_hat, self.layers[-1].get_activation().f_deriv
            )
            # calculate our weights and then update
            self.backward(theta)

            if adam:
                self.update_adam(k + 1, learning_rate, beta1, beta2)
            else:
                self.update(learning_rate, weight_decay, momentum)

            loss_vals[k] = np.mean(loss)
        logger.info(&#34;epoch &#34; + str(k + 1) + &#34; loss: &#34; + str(loss_vals[k]))
        # run callback if we have it
        if callback:
            callback(self, k)

    return loss_vals</code></pre>
</details>
</dd>
<dt id="DenserFlow.model.Model.__init__"><code class="name flex">
<span>def <span class="ident">__init__</span></span>(<span>self, loss_func)</span>
</code></dt>
<dd>
<section class="desc"><p>:param loss_func: The loss funcation to be used for this model.</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def __init__(self, loss_func: Loss):
    &#34;&#34;&#34;
    :param loss_func: The loss funcation to be used for this model.
    &#34;&#34;&#34;
    # initialize layers
    self.layers = []
    self.params = []
    self.error_func = loss_func</code></pre>
</details>
</dd>
<dt id="DenserFlow.model.Model.add_layer"><code class="name flex">
<span>def <span class="ident">add_layer</span></span>(<span>self, layer)</span>
</code></dt>
<dd>
<section class="desc"><p>Add a layer to the model.
:param layer: The layer to add.</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def add_layer(self, layer: Layer) -&gt; None:
    &#34;&#34;&#34;
    Add a layer to the model.
    :param layer: The layer to add.
    &#34;&#34;&#34;
    if self.layers:
        layer._add_prev_layer(
            self.layers[-1].get_activation(), self.layers[-1].out_dim
        )
    self.layers.append(layer)</code></pre>
</details>
</dd>
<dt id="DenserFlow.model.Model.backward"><code class="name flex">
<span>def <span class="ident">backward</span></span>(<span>self, delta)</span>
</code></dt>
<dd>
<section class="desc"><p>Perform a backward pass on the network, given the delta of the error function.
:param delta: The derivative of the error function of network with
respect to the net of the output layer.</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def backward(self, delta: Array[float]) -&gt; None:
    &#34;&#34;&#34;
    Perform a backward pass on the network, given the delta of the error function.
    :param delta: The derivative of the error function of network with
    respect to the net of the output layer.
    &#34;&#34;&#34;
    delta = self.layers[-1].backward(delta)
    for layer in reversed(self.layers[:-1]):
        delta = layer.backward(delta)</code></pre>
</details>
</dd>
<dt id="DenserFlow.model.Model.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, input_batch, mode=&#39;train&#39;)</span>
</code></dt>
<dd>
<section class="desc"><p>Perform a forward pass on the network for training.<br>
:param input_batch: The minibatched input. Must have shape (batch_size,)<br>
:param mode: Whether this is being used in train or test mode.</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def forward(self, input_batch: Array[float], mode: str = &#34;train&#34;) -&gt; Array[float]:
    &#34;&#34;&#34;
    Perform a forward pass on the network for training.
    :param input_batch: The minibatched input. Must have shape (batch_size,)
    :param mode: Whether this is being used in train or test mode.
    &#34;&#34;&#34;
    for layer in self.layers:
        output = layer.forward(input_batch, mode)
        input_batch = output
    return output</code></pre>
</details>
</dd>
<dt id="DenserFlow.model.Model.make_batch"><code class="name flex">
<span>def <span class="ident">make_batch</span></span>(<span>self, x, y, minibatch_size, shuffle=True)</span>
</code></dt>
<dd>
<section class="desc"><p>Makes minibatches from given data, and optionally shuffles them.</p>
<p>:param x: array of sample inputs<br>
:param y: array of sample labels<br>
:param minibatch_size: size of minibatches<br>
:param shuffle: Whether to shuffle the minibatches or not.</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def make_batch(
    self,
    x: Array[float],
    y: Array[float],
    minibatch_size: int,
    shuffle: bool = True,
) -&gt; List[Array[float]]:
    &#34;&#34;&#34;
    Makes minibatches from given data, and optionally shuffles them.

    :param x: array of sample inputs
    :param y: array of sample labels
    :param minibatch_size: size of minibatches
    :param shuffle: Whether to shuffle the minibatches or not.
    &#34;&#34;&#34;
    minibatches = []
    for idx in range(len(x) // minibatch_size):
        x_mini = []
        y_mini = []
        for i in range(idx, idx + minibatch_size):
            # Get pair of (X, y) of the current minibatch/chunk
            x_mini.append(x[i])
            y_mini.append(y[i])
        minibatches.append((np.array(x_mini), np.array(y_mini)))
    if shuffle:
        random.shuffle(minibatches)
    return minibatches</code></pre>
</details>
</dd>
<dt id="DenserFlow.model.Model.predict"><code class="name flex">
<span>def <span class="ident">predict</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<section class="desc"><p>Get the predictions of the model on a set of inputs.<br>
:param x: batch of inputs to feed into the model. Must have shape (batch_size,)</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def predict(self, x: Array[float]) -&gt; Array[float]:
    &#34;&#34;&#34;
    Get the predictions of the model on a set of inputs.
    :param x: batch of inputs to feed into the model. Must have shape (batch_size,)
    &#34;&#34;&#34;
    output = np.zeros((x.shape[0], self.layers[-1].out_dim))
    # for each sample
    for i in np.arange(x.shape[0]):
        output[i] = self.forward(x[i, :].reshape(1, -1), mode=&#34;test&#34;)
        # special case - we need to apply softmax without the loss function
        if type(self.error_func) is CrossEntropyWithSoftmax:
            output[i] = softmax().f(output[i])
    return output</code></pre>
</details>
</dd>
<dt id="DenserFlow.model.Model.update"><code class="name flex">
<span>def <span class="ident">update</span></span>(<span>self, lr, wd=0, m=0)</span>
</code></dt>
<dd>
<section class="desc"><p>Updates the weights in the network, using gradients calculated
in a backward pass.<br>
:param lr: learning rate to use<br>
:param wd: weight decay rate to use<br>
:param m: momentum rate to use</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def update(self, lr: float, wd: float = 0, m: float = 0) -&gt; None:
    &#34;&#34;&#34;
    Updates the weights in the network, using gradients calculated
    in a backward pass.
    :param lr: learning rate to use
    :param wd: weight decay rate to use
    :param m: momentum rate to use
    &#34;&#34;&#34;
    for layer in self.layers:
        layer.update(lr, wd, m)</code></pre>
</details>
</dd>
<dt id="DenserFlow.model.Model.update_adam"><code class="name flex">
<span>def <span class="ident">update_adam</span></span>(<span>self, t, alpha=0.001, beta1=0.9, beta2=0.999)</span>
</code></dt>
<dd>
<section class="desc"><p>Updates the weights in the network, using gradients calculated
in a backward pass.<br>
:param lr: learning rate to use<br>
:param wd: weight decay rate to use<br>
:param m: momentum rate to use</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def update_adam(
    self, t: float, alpha: float = 0.001, beta1: float = 0.9, beta2: float = 0.999
) -&gt; None:
    &#34;&#34;&#34;
    Updates the weights in the network, using gradients calculated
    in a backward pass.
    :param lr: learning rate to use
    :param wd: weight decay rate to use
    :param m: momentum rate to use
    &#34;&#34;&#34;
    for layer in self.layers:
        layer.update_adam(t, alpha, beta1, beta2)</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="DenserFlow" href="index.html">DenserFlow</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="DenserFlow.model.Model" href="#DenserFlow.model.Model">Model</a></code></h4>
<ul class="two-column">
<li><code><a title="DenserFlow.model.Model.__init__" href="#DenserFlow.model.Model.__init__">__init__</a></code></li>
<li><code><a title="DenserFlow.model.Model.SGD" href="#DenserFlow.model.Model.SGD">SGD</a></code></li>
<li><code><a title="DenserFlow.model.Model.add_layer" href="#DenserFlow.model.Model.add_layer">add_layer</a></code></li>
<li><code><a title="DenserFlow.model.Model.backward" href="#DenserFlow.model.Model.backward">backward</a></code></li>
<li><code><a title="DenserFlow.model.Model.forward" href="#DenserFlow.model.Model.forward">forward</a></code></li>
<li><code><a title="DenserFlow.model.Model.make_batch" href="#DenserFlow.model.Model.make_batch">make_batch</a></code></li>
<li><code><a title="DenserFlow.model.Model.predict" href="#DenserFlow.model.Model.predict">predict</a></code></li>
<li><code><a title="DenserFlow.model.Model.update" href="#DenserFlow.model.Model.update">update</a></code></li>
<li><code><a title="DenserFlow.model.Model.update_adam" href="#DenserFlow.model.Model.update_adam">update_adam</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.5.4</a>.</p>
</footer>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad()</script>
</body>
</html>